import fs from 'node:fs';
import csvParser from 'csv-parser';
import { pipeline, Transform } from 'stream';
import tableMapper from './tableMapperService';
import pool from '../database/db';
import { sanitizeIdentifier } from '../utils/utils';

/**
 * Service to process CSV files and insert into database tables
 */
class CsvProcessingService {
  /**
   * Process a single CSV file and insert into the matching table
   * @param {Object} file - File object from multer
   * @param {string} username - Current username
   * @returns {Promise<Object>} - Processing result
   */
  async processFile(file, username) {
    const startTime = new Date();
    const tableNameSanitized = sanitizeIdentifier(file.originalname);

    console.log(`üîÑ Processing file: ${file.originalname} - table: ${tableNameSanitized}`);

    try {
      // Determine target table from filename
      const tableName = tableMapper.getTableNameFromFilename(tableNameSanitized);

      if (!tableName) {
        throw new Error(`No matching table found for file: ${file.originalname}`);
      }

      if (!tableMapper.isValidTable(tableName)) {
        throw new Error(`Invalid table name: ${tableName}`);
      }

      console.log(`üìã Matched file to table: ${tableName}`);

      // Get column information for this table
      const columns = await tableMapper.getTableColumns(pool, tableName);

      if (!columns || columns.length === 0) {
        throw new Error(`No columns found for table: ${tableName}`);
      }

      // Process the CSV file
      const result = await this.importCsvToTable(file.path, tableName, columns, username);

      const endTime = new Date();
      const duration = (endTime - startTime) / 1000; // in seconds

      return {
        file: file.originalname,
        table: tableName,
        rowsProcessed: result.processedCount,
        rowsInserted: result.insertedCount,
        errors: result.errors,
        duration: duration,
        timestamp: '2025-04-07 02:17:31',
      };
    } catch (error) {
      console.error(`‚ùå Error processing file ${file.originalname}:`, error);
      return {
        file: file.originalname,
        error: error.message,
        timestamp: '2025-04-07 02:17:31',
      };
    }
  }

  /**
   * Import CSV data into specified table
   * @param {string} filePath - Path to CSV file
   * @param {string} tableName - Target table name
   * @param {Array} columns - Table column information
   * @param {string} username - Current username
   * @returns {Promise<Object>} - Import result
   */
  async importCsvToTable(filePath, tableName, columns, username) {
    let processedCount = 0;
    let insertedCount = 0;
    let errors = [];
    const batchSize = 1000;
    let currentBatch = [];

    // Create column name map for lookups
    const columnMap = new Map();
    columns.forEach(col => {
      columnMap.set(col.column_name, col);
    });

    // Set of columns to exclude from inserts
    const autoGenColumns = tableMapper.getAutoGeneratedColumns();

    // Transform stream for processing CSV rows
    const transformStream = new Transform({
      objectMode: true,
      transform(chunk, encoding, callback) {
        try {
          processedCount++;

          // Map CSV data to database columns
          const row = {};

          // Add username and timestamp info
          row['imported_by'] = username;

          // Process each field in the CSV row
          Object.entries(chunk).forEach(([header, value]) => {
            // Normalize the column name
            const normalizedColumn = tableMapper.normalizeColumnName(header);

            // Skip if column doesn't exist in the table
            if (!columnMap.has(normalizedColumn)) {
              return;
            }

            // Skip auto-generated columns
            if (autoGenColumns.has(normalizedColumn)) {
              return;
            }

            // Format the value based on column type
            const columnInfo = columnMap.get(normalizedColumn);
            row[normalizedColumn] = tableMapper.formatValueForType(value, columnInfo.data_type);
          });

          // Add to current batch
          currentBatch.push(row);

          // If batch is full, flush it
          if (currentBatch.length >= batchSize) {
            this.push({
              type: 'batch',
              data: [...currentBatch],
            });
            currentBatch = [];
          }

          callback();
        } catch (error) {
          errors.push({
            row: processedCount,
            error: error.message,
            data: chunk,
          });
          callback();
        }
      },
      flush(callback) {
        // Push any remaining rows
        if (currentBatch.length > 0) {
          this.push({
            type: 'batch',
            data: [...currentBatch],
          });
        }
        callback();
      },
    });

    // Create CSV parser stream
    const csvStream = csvParser();

    // Open file stream
    const fileStream = fs.createReadStream(filePath);

    // Start db client for transaction
    const client = await pool.connect();

    try {
      await client.query('BEGIN');

      // Process the stream and handle batches
      await new Promise((resolve, reject) => {
        pipeline(fileStream, csvStream, transformStream)
          .then(resolve)
          .catch(reject);

        transformStream.on('data', async (item) => {
          if (item.type === 'batch') {
            // Pause the stream while we insert
            transformStream.pause();

            try {
              const insertCount = await this.insertBatch(client, tableName, item.data, columns);
              insertedCount += insertCount;
            } catch (error) {
              console.error(`‚ùå Error inserting batch at row ${processedCount}:`, error);
              errors.push({
                row: processedCount,
                error: error.message,
                batch: true,
              });
            } finally {
              // Resume processing
              transformStream.resume();
            }
          }
        });
      });

      await client.query('COMMIT');

      console.log(`‚úÖ Imported ${insertedCount} rows to ${tableName} from CSV`);
    } catch (error) {
      await client.query('ROLLBACK');
      console.error(`‚ùå Transaction failed for ${tableName}:`, error);
      throw error;
    } finally {
      client.release();
    }

    return {
      processedCount,
      insertedCount,
      errors: errors.slice(0, 50), // Limit number of errors returned
    };
  }

  /**
   * Insert a batch of rows into the database
   * @param {Object} client - Database client
   * @param {string} tableName - Target table
   * @param {Array} rows - Rows to insert
   * @param {Array} columns - Table column information
   * @returns {Promise<number>} - Number of rows inserted
   */
  async insertBatch(client, tableName, rows, columns) {
    if (rows.length === 0) return 0;

    // Get column names from the first row
    const firstRow = rows[0];
    const columnNames = Object.keys(firstRow).filter(col => {
      // Exclude auto-generated columns
      const autoGenColumns = tableMapper.getAutoGeneratedColumns();
      return !autoGenColumns.has(col);
    });

    if (columnNames.length === 0) {
      return 0; // No valid columns to insert
    }

    // Build parameterized query
    const placeholders = [];
    const values = [];
    let paramCount = 1;

    rows.forEach(row => {
      const rowPlaceholders = [];

      columnNames.forEach(col => {
        rowPlaceholders.push(`$${paramCount}`);
        values.push(row[col]);
        paramCount++;
      });

      placeholders.push(`(${rowPlaceholders.join(', ')})`);
    });

    const query = `
        INSERT INTO "${tableName}" (${columnNames.map(c => `"${c}"`).join(', ')})
        VALUES
        ${placeholders.join(', ')}
      ON CONFLICT
        DO NOTHING
    `;

    const result = await client.query(query, values);
    return result.rowCount;
  }

  /**
   * Process multiple files in batch
   * @param {Array} files - Array of file objects
   * @param {string} username - Current username
   * @returns {Promise<Object>} - Batch result
   */
  async processBatch(files, username) {
    console.log(`üîÑ Starting batch CSV import at 2025-04-07 02:17:31 by wilmer2000`);
    console.log(`üì¶ Processing ${files.length} files`);

    const startTime = new Date();
    const results = [];

    // Process files sequentially to avoid database connection issues
    for (const file of files) {
      try {
        const result = await this.processFile(file, username);
        results.push(result);
      } catch (error) {
        console.error(`‚ùå Error processing file ${file.originalname}:`, error);
        results.push({
          file: file.originalname,
          error: error.message,
          timestamp: '2025-04-07 02:17:31',
        });
      }
    }

    const endTime = new Date();
    const duration = (endTime - startTime) / 1000; // in seconds

    const successful = results.filter(r => !r.error).length;
    const failed = results.filter(r => r.error).length;

    console.log(`‚úÖ Batch import completed: ${successful} successful, ${failed} failed`);
    console.log(`‚è±Ô∏è Total duration: ${duration} seconds`);

    return {
      totalFiles: files.length,
      successful,
      failed,
      duration,
      results,
      timestamp: '2025-04-07 02:17:31',
    };
  }
}

export default new CsvProcessingService();